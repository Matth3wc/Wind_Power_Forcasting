{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler  # ADD THIS LINE\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')\n",
    "sys.path.append('../src')\n",
    "from irish_buoy_data import IrishBuoyData\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# %%\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add Numba imports here\n",
    "from numba import jit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tensor created!\n",
      "Shape: (44200, 5, 6)\n",
      "  - Timesteps: 44200\n",
      "  - Buoys: 5\n",
      "  - Features: 6\n",
      "\n",
      "=== Verification ===\n",
      "Feature names: ['WindSpeed (knots)', 'AirTemperature (degrees_C)', 'AtmosphericPressure (millibars)', 'WaveHeight (meters)', 'Hmax (meters)', 'Tp (seconds)']\n",
      "\n",
      "M3 state at first timestep:\n",
      "[   8.425    9.824 1008.801    3.516    6.68    11.367]\n",
      "\n",
      "DataFrame equivalent:\n",
      "WindSpeed (knots)                     8.425\n",
      "AirTemperature (degrees_C)            9.824\n",
      "AtmosphericPressure (millibars)    1008.801\n",
      "WaveHeight (meters)                   3.516\n",
      "Hmax (meters)                         6.680\n",
      "Tp (seconds)                         11.367\n",
      "Name: 2020-11-11 00:00:00+00:00, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/0q6grdh110z1c008s2nrczkm0000gn/T/ipykernel_73310/4109033216.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_multi = df_multi.interpolate(method='time', limit=3).fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get your multi-level data (you already have this)\n",
    "buoy_stations = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "all_buoy_data = []\n",
    "\n",
    "for station_id in buoy_stations:\n",
    "    try:\n",
    "        buoy = IrishBuoyData(station_id=station_id)\n",
    "        data = buoy.fetch_data(days_back=1875)\n",
    "        data = data[~data.index.duplicated(keep='first')]\n",
    "        data = data.drop(columns=['station_id'])\n",
    "        all_buoy_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {station_id}: Error - {e}\")\n",
    "\n",
    "# Create multi-level DataFrame\n",
    "df_multi = pd.concat(all_buoy_data, axis=1, keys=buoy_stations)\n",
    "df_multi = df_multi.interpolate(method='time', limit=3).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Step 2: Convert to 3D tensor [time, buoys, features]\n",
    "def multi_to_tensor(df_multi):\n",
    "    \"\"\"\n",
    "    Convert multi-level DataFrame to 3D tensor\n",
    "    Structure: [timesteps, buoys, features]\n",
    "    \"\"\"\n",
    "    buoy_ids = df_multi.columns.levels[0].tolist()\n",
    "    n_timesteps = len(df_multi)\n",
    "    n_buoys = len(buoy_ids)\n",
    "    n_features = len(df_multi[buoy_ids[0]].columns)\n",
    "    \n",
    "    # Initialize tensor\n",
    "    X_tensor = np.zeros((n_timesteps, n_buoys, n_features))\n",
    "    \n",
    "    # Fill tensor\n",
    "    for b_idx, buoy in enumerate(buoy_ids):\n",
    "        X_tensor[:, b_idx, :] = df_multi[buoy].values\n",
    "    \n",
    "    return X_tensor\n",
    "\n",
    "# Create the tensor\n",
    "X_tensor = multi_to_tensor(df_multi)\n",
    "\n",
    "print(f\"✓ Tensor created!\")\n",
    "print(f\"Shape: {X_tensor.shape}\")\n",
    "print(f\"  - Timesteps: {X_tensor.shape[0]}\")\n",
    "print(f\"  - Buoys: {X_tensor.shape[1]}\")\n",
    "print(f\"  - Features: {X_tensor.shape[2]}\")\n",
    "\n",
    "# Verify structure\n",
    "print(f\"\\n=== Verification ===\")\n",
    "print(f\"Feature names: {df_multi['M2'].columns.tolist()}\")\n",
    "print(f\"\\nM3 state at first timestep:\")\n",
    "print(X_tensor[0, 2, :])  # buoy index 2 = M3\n",
    "\n",
    "print(f\"\\nDataFrame equivalent:\")\n",
    "print(df_multi['M4'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# ============================================================================\n",
    "# Simple LSTM Model for Buoy State Prediction\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Simple Dataset (No External Forcing)\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleBuoyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset: predict X_{t+1} from [X_{t-n}, ..., X_t]\n",
    "    \"\"\"\n",
    "    def __init__(self, X_tensor, sequence_length=24):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_tensor: (timesteps, n_buoys, n_features)\n",
    "            sequence_length: how many past timesteps to use\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X_tensor)\n",
    "        self.seq_len = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: past sequence [X_{t-n}, ..., X_t]\n",
    "        X_history = self.X[idx:idx+self.seq_len]  # (seq_len, n_buoys, n_features)\n",
    "        \n",
    "        # Target: next state X_{t+1}\n",
    "        X_target = self.X[idx+self.seq_len]  # (n_buoys, n_features)\n",
    "        \n",
    "        return X_history, X_target\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Simple LSTM Model\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple LSTM: X_{t+1} = f(X_t, X_{t-1}, ..., X_{t-n})\n",
    "    \n",
    "    Architecture:\n",
    "    1. Flatten buoy dimensions\n",
    "    2. LSTM processes sequence\n",
    "    3. FC layer predicts next state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_buoys=5, n_features=6, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_buoys = n_buoys\n",
    "        self.n_features = n_features\n",
    "        self.input_dim = n_buoys * n_features  # Flatten: 5 * 6 = 30\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, self.input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, n_buoys, n_features)\n",
    "        Returns:\n",
    "            prediction: (batch, n_buoys, n_features)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_buoys, n_features = x.shape\n",
    "        \n",
    "        # Flatten spatial dimensions: (batch, seq_len, n_buoys*n_features)\n",
    "        x_flat = x.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # LSTM forward\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x_flat)\n",
    "        \n",
    "        # Use final hidden state\n",
    "        final_hidden = h_n[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict next state\n",
    "        output = self.fc(final_hidden)  # (batch, n_buoys*n_features)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.reshape(batch_size, n_buoys, n_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Fast Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_lstm(model, train_loader, val_loader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    \"\"\"Simple, fast training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_history, X_target in train_loader:\n",
    "            X_history = X_history.to(device)\n",
    "            X_target = X_target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X_history)\n",
    "            loss = criterion(pred, X_target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_history, X_target in val_loader:\n",
    "                X_history = X_history.to(device)\n",
    "                X_target = X_target.to(device)\n",
    "                \n",
    "                pred = model(X_history)\n",
    "                loss = criterion(pred, X_target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.6f}, Val: {val_loss:.6f} - {epoch_time:.1f}s\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Setup and Train\n",
    "# ============================================================================\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# COMPLETE LSTM FORECASTING WITH NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LSTM MODEL FOR BUOY FORECASTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Get recent data\n",
    "recent_size = 180 * 24  # 6 months\n",
    "X_tensor_recent = X_tensor[-recent_size:]\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X_tensor_recent.shape}\")\n",
    "print(f\"Original stats - Mean: {X_tensor_recent.mean():.2f}, Std: {X_tensor_recent.std():.2f}\")\n",
    "\n",
    "# Step 2: NORMALIZE DATA (CRITICAL!)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_timesteps, n_buoys, n_features = X_tensor_recent.shape\n",
    "scalers = []\n",
    "X_normalized = np.zeros_like(X_tensor_recent)\n",
    "\n",
    "feature_names = ['WindSpeed', 'AirTemp', 'Pressure', 'WaveHeight', 'Hmax', 'Tp']\n",
    "\n",
    "print(\"\\nNormalizing features...\")\n",
    "for f_idx in range(n_features):\n",
    "    feature_values = X_tensor_recent[:, :, f_idx].reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_values = scaler.fit_transform(feature_values)\n",
    "    X_normalized[:, :, f_idx] = normalized_values.reshape(n_timesteps, n_buoys)\n",
    "    scalers.append(scaler)\n",
    "    print(f\"  {feature_names[f_idx]:12s} - normalized\")\n",
    "\n",
    "print(f\"Normalized stats - Mean: {X_normalized.mean():.6f}, Std: {X_normalized.std():.6f}\")\n",
    "\n",
    "# Step 3: Create dataset with NORMALIZED data\n",
    "sequence_length = 24\n",
    "dataset = SimpleBuoyDataset(X_normalized, sequence_length=sequence_length)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Step 4: Split data\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(dataset)))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Step 5: Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Step 6: Initialize model\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "model = SimpleLSTM(n_buoys=5, n_features=6, hidden_dim=128, num_layers=2)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# Step 7: Train\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_losses, val_losses = train_lstm(\n",
    "    model, train_loader, val_loader, \n",
    "    num_epochs=20, \n",
    "    lr=0.001,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# Step 8: Generate predictions (normalized)\n",
    "print(\"\\nGenerating predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_history, X_target in val_loader:\n",
    "        X_history = X_history.to(device)\n",
    "        pred = model(X_history)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(X_target.numpy())\n",
    "\n",
    "predictions_normalized = np.vstack(all_preds)\n",
    "actuals_normalized = np.vstack(all_targets)\n",
    "\n",
    "print(f\"Predictions shape: {predictions_normalized.shape}\")\n",
    "\n",
    "# Step 9: Denormalize predictions\n",
    "print(\"Denormalizing predictions...\")\n",
    "predictions = np.zeros_like(predictions_normalized)\n",
    "actuals = np.zeros_like(actuals_normalized)\n",
    "\n",
    "for f_idx in range(n_features):\n",
    "    # Denormalize predictions\n",
    "    pred_flat = predictions_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    predictions[:, :, f_idx] = scalers[f_idx].inverse_transform(pred_flat).reshape(-1, n_buoys)\n",
    "    \n",
    "    # Denormalize actuals\n",
    "    actual_flat = actuals_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    actuals[:, :, f_idx] = scalers[f_idx].inverse_transform(actual_flat).reshape(-1, n_buoys)\n",
    "\n",
    "print(\"✓ Predictions denormalized\")\n",
    "\n",
    "# Step 10: Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "buoy_names = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "\n",
    "for b_idx, buoy in enumerate(buoy_names):\n",
    "    print(f\"\\n{buoy}:\")\n",
    "    for f_idx, feature in enumerate(feature_names):\n",
    "        actual = actuals[:, b_idx, f_idx]\n",
    "        pred = predictions[:, b_idx, f_idx]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        r2 = r2_score(actual, pred)\n",
    "        \n",
    "        print(f\"  {feature:<15} RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Step 11: Plot results\n",
    "print(\"\\nPlotting results...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Training curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual (M5, WindSpeed)\n",
    "b_idx = 3  # M5\n",
    "f_idx = 0  # WindSpeed\n",
    "plot_samples = min(168, len(actuals))  # Last week\n",
    "\n",
    "axes[1].plot(actuals[-plot_samples:, b_idx, f_idx], 'o-', label='Actual', \n",
    "             markersize=4, linewidth=2, color='steelblue')\n",
    "axes[1].plot(predictions[-plot_samples:, b_idx, f_idx], 's-', label='Predicted', \n",
    "             markersize=3, alpha=0.7, linewidth=2, color='coral')\n",
    "axes[1].set_xlabel('Time (hours)')\n",
    "axes[1].set_ylabel('Wind Speed (knots)')\n",
    "axes[1].set_title('M5 Wind Speed - Last Week Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Step 12: Prediction quality check\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prediction variance: {predictions[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Actual variance: {actuals[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Variance ratio: {predictions[:, b_idx, f_idx].std() / actuals[:, b_idx, f_idx].std():.4f}\")\n",
    "\n",
    "# Step 13: Save model\n",
    "torch.save(model.state_dict(), 'simple_lstm_buoy_model.pth')\n",
    "print(\"\\n✓ Model saved to 'simple_lstm_buoy_model.pth'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# ============================================================================\n",
    "# FIX: NORMALIZE DATA BEFORE TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NORMALIZING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Your current data\n",
    "recent_size = 180 * 24  # 6 months\n",
    "X_tensor_recent = X_tensor[-recent_size:]\n",
    "\n",
    "print(f\"Original data shape: {X_tensor_recent.shape}\")\n",
    "print(f\"Original stats - Mean: {X_tensor_recent.mean():.2f}, Std: {X_tensor_recent.std():.2f}\")\n",
    "print(f\"Original range: [{X_tensor_recent.min():.2f}, {X_tensor_recent.max():.2f}]\")\n",
    "\n",
    "# NORMALIZE PROPERLY\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_timesteps, n_buoys, n_features = X_tensor_recent.shape\n",
    "\n",
    "# Store scalers for each feature (to denormalize later)\n",
    "scalers = []\n",
    "X_normalized = np.zeros_like(X_tensor_recent)\n",
    "\n",
    "feature_names = ['WindSpeed', 'AirTemp', 'Pressure', 'WaveHeight', 'Hmax', 'Tp']\n",
    "\n",
    "for f_idx in range(n_features):\n",
    "    # Extract all values for this feature\n",
    "    feature_values = X_tensor_recent[:, :, f_idx].reshape(-1, 1)\n",
    "    \n",
    "    # Fit scaler\n",
    "    scaler = StandardScaler()\n",
    "    normalized_values = scaler.fit_transform(feature_values)\n",
    "    \n",
    "    # Reshape back\n",
    "    X_normalized[:, :, f_idx] = normalized_values.reshape(n_timesteps, n_buoys)\n",
    "    \n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    print(f\"{feature_names[f_idx]:12s} - Original: [{feature_values.min():.2f}, {feature_values.max():.2f}], \"\n",
    "          f\"Normalized: [{normalized_values.min():.2f}, {normalized_values.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nNormalized stats - Mean: {X_normalized.mean():.6f}, Std: {X_normalized.std():.6f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# NOW CREATE DATASET WITH NORMALIZED DATA\n",
    "# ============================================================================\n",
    "\n",
    "sequence_length = 24\n",
    "dataset = SimpleBuoyDataset(X_normalized, sequence_length=sequence_length)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(dataset)))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RETRAIN WITH NORMALIZED DATA\n",
    "# ============================================================================\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = SimpleLSTM(n_buoys=5, n_features=6, hidden_dim=128, num_layers=2)\n",
    "\n",
    "print(f\"\\nRetraining with normalized data...\")\n",
    "\n",
    "train_losses, val_losses = train_lstm(\n",
    "    model, train_loader, val_loader, \n",
    "    num_epochs=20, \n",
    "    lr=0.01,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# DENORMALIZE PREDICTIONS FOR EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_history, X_target in val_loader:\n",
    "        X_history = X_history.to(device)\n",
    "        pred = model(X_history)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(X_target.numpy())\n",
    "\n",
    "predictions_normalized = np.vstack(all_preds)\n",
    "actuals_normalized = np.vstack(all_targets)\n",
    "\n",
    "# Denormalize\n",
    "predictions = np.zeros_like(predictions_normalized)\n",
    "actuals = np.zeros_like(actuals_normalized)\n",
    "\n",
    "for f_idx in range(n_features):\n",
    "    # Predictions\n",
    "    pred_flat = predictions_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    pred_denorm = scalers[f_idx].inverse_transform(pred_flat)\n",
    "    predictions[:, :, f_idx] = pred_denorm.reshape(-1, n_buoys)\n",
    "    \n",
    "    # Actuals\n",
    "    actual_flat = actuals_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    actual_denorm = scalers[f_idx].inverse_transform(actual_flat)\n",
    "    actuals[:, :, f_idx] = actual_denorm.reshape(-1, n_buoys)\n",
    "\n",
    "print(f\"\\n✓ Predictions denormalized\")\n",
    "\n",
    "# ============================================================================\n",
    "# RECALCULATE METRICS WITH DENORMALIZED DATA\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS (DENORMALIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "buoy_names = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "\n",
    "for b_idx, buoy in enumerate(buoy_names):\n",
    "    print(f\"\\n{buoy}:\")\n",
    "    for f_idx, feature in enumerate(feature_names):\n",
    "        actual = actuals[:, b_idx, f_idx]\n",
    "        pred = predictions[:, b_idx, f_idx]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        r2 = r2_score(actual, pred)\n",
    "        \n",
    "        print(f\"  {feature:<15} RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot M5 Wind Speed\n",
    "b_idx = 3  # M5\n",
    "f_idx = 0  # WindSpeed\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_samples = min(168, len(actuals))  # Last week or all data\n",
    "\n",
    "plt.plot(actuals[-plot_samples:, b_idx, f_idx], 'o-', label='Actual', markersize=4, linewidth=2)\n",
    "plt.plot(predictions[-plot_samples:, b_idx, f_idx], 's-', label='Predicted', markersize=3, alpha=0.7, linewidth=2)\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Wind Speed (knots)')\n",
    "plt.title('M5 Wind Speed - Predictions vs Actual')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check variance\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prediction variance: {predictions[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Actual variance: {actuals[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Ratio: {predictions[:, b_idx, f_idx].std() / actuals[:, b_idx, f_idx].std():.4f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model classes defined\n",
      "======================================================================\n",
      "LSTM MODEL FOR BUOY FORECASTING\n",
      "======================================================================\n",
      "\n",
      "Data shape: (44200, 5, 6)\n",
      "Original stats - Mean: 175.95, Std: 374.36\n",
      "\n",
      "Normalizing features...\n",
      "  WindSpeed    - normalized\n",
      "  AirTemp      - normalized\n",
      "  Pressure     - normalized\n",
      "  WaveHeight   - normalized\n",
      "  Hmax         - normalized\n",
      "  Tp           - normalized\n",
      "Normalized stats - Mean: -0.000000, Std: 1.000000\n",
      "\n",
      "Dataset size: 44176 samples\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 287\u001b[0m\n\u001b[1;32m    284\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(train_size))\n\u001b[1;32m    285\u001b[0m val_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(train_size, \u001b[38;5;28mlen\u001b[39m(dataset)))\n\u001b[0;32m--> 287\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSubset\u001b[49m(dataset, train_indices)\n\u001b[1;32m    288\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m Subset(dataset, val_indices)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Subset' is not defined"
     ]
    }
   ],
   "source": [
    "'''# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')\n",
    "sys.path.append('../src')\n",
    "from irish_buoy_data import IrishBuoyData\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# %% [markdown]\n",
    "# # Preparing the Data Frame\n",
    "\n",
    "# %%\n",
    "# Step 1: Get your multi-level data\n",
    "buoy_stations = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "all_buoy_data = []\n",
    "\n",
    "print(\"Fetching data from all buoys...\")\n",
    "for station_id in buoy_stations:\n",
    "    try:\n",
    "        buoy = IrishBuoyData(station_id=station_id)\n",
    "        data = buoy.fetch_data(days_back=365)  # 1 year for faster testing\n",
    "        data = data[~data.index.duplicated(keep='first')]\n",
    "        data = data.drop(columns=['station_id'], errors='ignore')\n",
    "        all_buoy_data.append(data)\n",
    "        print(f\"  ✓ {station_id}: {len(data)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {station_id}: Error - {e}\")\n",
    "\n",
    "# Create multi-level DataFrame\n",
    "df_multi = pd.concat(all_buoy_data, axis=1, keys=buoy_stations)\n",
    "df_multi = df_multi.interpolate(method='time', limit=3).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "print(f\"\\n✓ DataFrame created: {df_multi.shape}\")\n",
    "\n",
    "# %%\n",
    "# Step 2: Convert to 3D tensor [time, buoys, features]\n",
    "def multi_to_tensor(df_multi):\n",
    "    \"\"\"\n",
    "    Convert multi-level DataFrame to 3D tensor\n",
    "    Structure: [timesteps, buoys, features]\n",
    "    \"\"\"\n",
    "    buoy_ids = df_multi.columns.levels[0].tolist()\n",
    "    n_timesteps = len(df_multi)\n",
    "    n_buoys = len(buoy_ids)\n",
    "    n_features = len(df_multi[buoy_ids[0]].columns)\n",
    "    \n",
    "    # Initialize tensor\n",
    "    X_tensor = np.zeros((n_timesteps, n_buoys, n_features))\n",
    "    \n",
    "    # Fill tensor\n",
    "    for b_idx, buoy in enumerate(buoy_ids):\n",
    "        X_tensor[:, b_idx, :] = df_multi[buoy].values\n",
    "    \n",
    "    return X_tensor\n",
    "\n",
    "# Create the tensor\n",
    "X_tensor = multi_to_tensor(df_multi)\n",
    "\n",
    "print(f\"\\n✓ Tensor created!\")\n",
    "print(f\"Shape: {X_tensor.shape}\")\n",
    "print(f\"  - Timesteps: {X_tensor.shape[0]}\")\n",
    "print(f\"  - Buoys: {X_tensor.shape[1]}\")\n",
    "print(f\"  - Features: {X_tensor.shape[2]}\")\n",
    "\n",
    "# Verify structure\n",
    "print(f\"\\n=== Verification ===\")\n",
    "print(f\"Feature names: {df_multi['M2'].columns.tolist()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Define LSTM Model\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# LSTM Model Classes and Functions\n",
    "# ============================================================================'''\n",
    "\n",
    "class SimpleBuoyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset: predict X_{t+1} from [X_{t-n}, ..., X_t]\n",
    "    \"\"\"\n",
    "    def __init__(self, X_tensor, sequence_length=24):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_tensor: (timesteps, n_buoys, n_features)\n",
    "            sequence_length: how many past timesteps to use\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X_tensor)\n",
    "        self.seq_len = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: past sequence [X_{t-n}, ..., X_t]\n",
    "        X_history = self.X[idx:idx+self.seq_len]  # (seq_len, n_buoys, n_features)\n",
    "        \n",
    "        # Target: next state X_{t+1}\n",
    "        X_target = self.X[idx+self.seq_len]  # (n_buoys, n_features)\n",
    "        \n",
    "        return X_history, X_target\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple LSTM: X_{t+1} = f(X_t, X_{t-1}, ..., X_{t-n})\n",
    "    \n",
    "    Architecture:\n",
    "    1. Flatten buoy dimensions\n",
    "    2. LSTM processes sequence\n",
    "    3. FC layer predicts next state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_buoys=5, n_features=6, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_buoys = n_buoys\n",
    "        self.n_features = n_features\n",
    "        self.input_dim = n_buoys * n_features  # Flatten: 5 * 6 = 30\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, self.input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, n_buoys, n_features)\n",
    "        Returns:\n",
    "            prediction: (batch, n_buoys, n_features)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_buoys, n_features = x.shape\n",
    "        \n",
    "        # Flatten spatial dimensions: (batch, seq_len, n_buoys*n_features)\n",
    "        x_flat = x.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # LSTM forward\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x_flat)\n",
    "        \n",
    "        # Use final hidden state\n",
    "        final_hidden = h_n[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict next state\n",
    "        output = self.fc(final_hidden)  # (batch, n_buoys*n_features)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.reshape(batch_size, n_buoys, n_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def train_lstm(model, train_loader, val_loader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    \"\"\"Simple, fast training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_history, X_target in train_loader:\n",
    "            X_history = X_history.to(device)\n",
    "            X_target = X_target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X_history)\n",
    "            loss = criterion(pred, X_target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_history, X_target in val_loader:\n",
    "                X_history = X_history.to(device)\n",
    "                X_target = X_target.to(device)\n",
    "                \n",
    "                pred = model(X_history)\n",
    "                loss = criterion(pred, X_target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.6f}, Val: {val_loss:.6f} - {epoch_time:.1f}s\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"✓ Model classes defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Train Model with Normalization\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# COMPLETE LSTM FORECASTING WITH NORMALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LSTM MODEL FOR BUOY FORECASTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Get recent data (use all available data or subset)\n",
    "X_tensor_recent = X_tensor  # Use all data\n",
    "\n",
    "print(f\"\\nData shape: {X_tensor_recent.shape}\")\n",
    "print(f\"Original stats - Mean: {X_tensor_recent.mean():.2f}, Std: {X_tensor_recent.std():.2f}\")\n",
    "\n",
    "# Step 2: NORMALIZE DATA (CRITICAL!)\n",
    "n_timesteps, n_buoys, n_features = X_tensor_recent.shape\n",
    "scalers = []\n",
    "X_normalized = np.zeros_like(X_tensor_recent)\n",
    "\n",
    "feature_names = ['WindSpeed', 'AirTemp', 'Pressure', 'WaveHeight', 'Hmax', 'Tp']\n",
    "\n",
    "print(\"\\nNormalizing features...\")\n",
    "for f_idx in range(n_features):\n",
    "    feature_values = X_tensor_recent[:, :, f_idx].reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_values = scaler.fit_transform(feature_values)\n",
    "    X_normalized[:, :, f_idx] = normalized_values.reshape(n_timesteps, n_buoys)\n",
    "    scalers.append(scaler)\n",
    "    print(f\"  {feature_names[f_idx]:12s} - normalized\")\n",
    "\n",
    "print(f\"Normalized stats - Mean: {X_normalized.mean():.6f}, Std: {X_normalized.std():.6f}\")\n",
    "\n",
    "# Step 3: Create dataset with NORMALIZED data\n",
    "sequence_length = 24\n",
    "dataset = SimpleBuoyDataset(X_normalized, sequence_length=sequence_length)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Step 4: Split data (80/20)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(dataset)))\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Step 5: Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Step 6: Initialize model\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "model = SimpleLSTM(n_buoys=5, n_features=6, hidden_dim=128, num_layers=2)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# Step 7: Train\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_losses, val_losses = train_lstm(\n",
    "    model, train_loader, val_loader, \n",
    "    num_epochs=20, \n",
    "    lr=0.001,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# # Evaluate and Visualize Results\n",
    "\n",
    "# %%\n",
    "# Step 8: Generate predictions (normalized)\n",
    "print(\"Generating predictions...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_history, X_target in val_loader:\n",
    "        X_history = X_history.to(device)\n",
    "        pred = model(X_history)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(X_target.numpy())\n",
    "\n",
    "predictions_normalized = np.vstack(all_preds)\n",
    "actuals_normalized = np.vstack(all_targets)\n",
    "\n",
    "print(f\"Predictions shape: {predictions_normalized.shape}\")\n",
    "\n",
    "# Step 9: Denormalize predictions\n",
    "print(\"Denormalizing predictions...\")\n",
    "predictions = np.zeros_like(predictions_normalized)\n",
    "actuals = np.zeros_like(actuals_normalized)\n",
    "\n",
    "for f_idx in range(n_features):\n",
    "    # Denormalize predictions\n",
    "    pred_flat = predictions_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    predictions[:, :, f_idx] = scalers[f_idx].inverse_transform(pred_flat).reshape(-1, n_buoys)\n",
    "    \n",
    "    # Denormalize actuals\n",
    "    actual_flat = actuals_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    actuals[:, :, f_idx] = scalers[f_idx].inverse_transform(actual_flat).reshape(-1, n_buoys)\n",
    "\n",
    "print(\"✓ Predictions denormalized\")\n",
    "\n",
    "# Step 10: Calculate metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "buoy_names = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "\n",
    "for b_idx, buoy in enumerate(buoy_names):\n",
    "    print(f\"\\n{buoy}:\")\n",
    "    for f_idx, feature in enumerate(feature_names):\n",
    "        actual = actuals[:, b_idx, f_idx]\n",
    "        pred = predictions[:, b_idx, f_idx]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        r2 = r2_score(actual, pred)\n",
    "        \n",
    "        print(f\"  {feature:<15} RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Step 11: Plot results\n",
    "print(\"\\nPlotting results...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Training curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions vs Actual (M5, WindSpeed)\n",
    "b_idx = 3  # M5\n",
    "f_idx = 0  # WindSpeed\n",
    "plot_samples = min(168, len(actuals))  # Last week\n",
    "\n",
    "axes[1].plot(actuals[-plot_samples:, b_idx, f_idx], 'o-', label='Actual', \n",
    "             markersize=4, linewidth=2, color='steelblue')\n",
    "axes[1].plot(predictions[-plot_samples:, b_idx, f_idx], 's-', label='Predicted', \n",
    "             markersize=3, alpha=0.7, linewidth=2, color='coral')\n",
    "axes[1].set_xlabel('Time (hours)')\n",
    "axes[1].set_ylabel('Wind Speed (knots)')\n",
    "axes[1].set_title('M5 Wind Speed - Predictions vs Actual')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Step 12: Prediction quality check\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prediction variance: {predictions[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Actual variance: {actuals[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Variance ratio: {predictions[:, b_idx, f_idx].std() / actuals[:, b_idx, f_idx].std():.4f}\")\n",
    "\n",
    "# Step 13: Save model\n",
    "torch.save(model.state_dict(), 'simple_lstm_buoy_model.pth')\n",
    "print(\"\\n✓ Model saved to 'simple_lstm_buoy_model.pth'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
