{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler  # ADD THIS LINE\n",
    "color_pal = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')\n",
    "sys.path.append('../src')\n",
    "from irish_buoy_data import IrishBuoyData\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# %%\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add Numba imports here\n",
    "from numba import jit, prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tensor created!\n",
      "Shape: (44200, 5, 6)\n",
      "  - Timesteps: 44200\n",
      "  - Buoys: 5\n",
      "  - Features: 6\n",
      "\n",
      "=== Verification ===\n",
      "Feature names: ['WindSpeed (knots)', 'AirTemperature (degrees_C)', 'AtmosphericPressure (millibars)', 'WaveHeight (meters)', 'Hmax (meters)', 'Tp (seconds)']\n",
      "\n",
      "M3 state at first timestep:\n",
      "[  14.117    9.775 1001.16     3.125    4.805   12.539]\n",
      "\n",
      "DataFrame equivalent:\n",
      "WindSpeed (knots)                    14.117\n",
      "AirTemperature (degrees_C)            9.775\n",
      "AtmosphericPressure (millibars)    1001.160\n",
      "WaveHeight (meters)                   3.125\n",
      "Hmax (meters)                         4.805\n",
      "Tp (seconds)                         12.539\n",
      "Name: 2020-11-11 23:00:00+00:00, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1j/0q6grdh110z1c008s2nrczkm0000gn/T/ipykernel_98716/4109033216.py:17: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_multi = df_multi.interpolate(method='time', limit=3).fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get your multi-level data (you already have this)\n",
    "buoy_stations = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "all_buoy_data = []\n",
    "\n",
    "for station_id in buoy_stations:\n",
    "    try:\n",
    "        buoy = IrishBuoyData(station_id=station_id)\n",
    "        data = buoy.fetch_data(days_back=1875)\n",
    "        data = data[~data.index.duplicated(keep='first')]\n",
    "        data = data.drop(columns=['station_id'])\n",
    "        all_buoy_data.append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ {station_id}: Error - {e}\")\n",
    "\n",
    "# Create multi-level DataFrame\n",
    "df_multi = pd.concat(all_buoy_data, axis=1, keys=buoy_stations)\n",
    "df_multi = df_multi.interpolate(method='time', limit=3).fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Step 2: Convert to 3D tensor [time, buoys, features]\n",
    "def multi_to_tensor(df_multi):\n",
    "    \"\"\"\n",
    "    Convert multi-level DataFrame to 3D tensor\n",
    "    Structure: [timesteps, buoys, features]\n",
    "    \"\"\"\n",
    "    buoy_ids = df_multi.columns.levels[0].tolist()\n",
    "    n_timesteps = len(df_multi)\n",
    "    n_buoys = len(buoy_ids)\n",
    "    n_features = len(df_multi[buoy_ids[0]].columns)\n",
    "    \n",
    "    # Initialize tensor\n",
    "    X_tensor = np.zeros((n_timesteps, n_buoys, n_features))\n",
    "    \n",
    "    # Fill tensor\n",
    "    for b_idx, buoy in enumerate(buoy_ids):\n",
    "        X_tensor[:, b_idx, :] = df_multi[buoy].values\n",
    "    \n",
    "    return X_tensor\n",
    "\n",
    "# Create the tensor\n",
    "X_tensor = multi_to_tensor(df_multi)\n",
    "\n",
    "print(f\"✓ Tensor created!\")\n",
    "print(f\"Shape: {X_tensor.shape}\")\n",
    "print(f\"  - Timesteps: {X_tensor.shape[0]}\")\n",
    "print(f\"  - Buoys: {X_tensor.shape[1]}\")\n",
    "print(f\"  - Features: {X_tensor.shape[2]}\")\n",
    "\n",
    "# Verify structure\n",
    "print(f\"\\n=== Verification ===\")\n",
    "print(f\"Feature names: {df_multi['M2'].columns.tolist()}\")\n",
    "print(f\"\\nM3 state at first timestep:\")\n",
    "print(X_tensor[0, 2, :])  # buoy index 2 = M3\n",
    "\n",
    "print(f\"\\nDataFrame equivalent:\")\n",
    "print(df_multi['M4'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Simple LSTM Model for Buoy State Prediction\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Simple Dataset (No External Forcing)\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleBuoyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset: predict X_{t+1} from [X_{t-n}, ..., X_t]\n",
    "    \"\"\"\n",
    "    def __init__(self, X_tensor, sequence_length=24):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_tensor: (timesteps, n_buoys, n_features)\n",
    "            sequence_length: how many past timesteps to use\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X_tensor)\n",
    "        self.seq_len = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: past sequence [X_{t-n}, ..., X_t]\n",
    "        X_history = self.X[idx:idx+self.seq_len]  # (seq_len, n_buoys, n_features)\n",
    "        \n",
    "        # Target: next state X_{t+1}\n",
    "        X_target = self.X[idx+self.seq_len]  # (n_buoys, n_features)\n",
    "        \n",
    "        return X_history, X_target\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Simple LSTM Model\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple LSTM: X_{t+1} = f(X_t, X_{t-1}, ..., X_{t-n})\n",
    "    \n",
    "    Architecture:\n",
    "    1. Flatten buoy dimensions\n",
    "    2. LSTM processes sequence\n",
    "    3. FC layer predicts next state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_buoys=5, n_features=6, hidden_dim=128, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_buoys = n_buoys\n",
    "        self.n_features = n_features\n",
    "        self.input_dim = n_buoys * n_features  # Flatten: 5 * 6 = 30\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, self.input_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, n_buoys, n_features)\n",
    "        Returns:\n",
    "            prediction: (batch, n_buoys, n_features)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_buoys, n_features = x.shape\n",
    "        \n",
    "        # Flatten spatial dimensions: (batch, seq_len, n_buoys*n_features)\n",
    "        x_flat = x.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # LSTM forward\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x_flat)\n",
    "        \n",
    "        # Use final hidden state\n",
    "        final_hidden = h_n[-1]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict next state\n",
    "        output = self.fc(final_hidden)  # (batch, n_buoys*n_features)\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.reshape(batch_size, n_buoys, n_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Fast Training Function\n",
    "# ============================================================================\n",
    "\n",
    "def train_lstm(model, train_loader, val_loader, num_epochs=20, lr=0.001, device='cpu'):\n",
    "    \"\"\"Simple, fast training loop\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_history, X_target in train_loader:\n",
    "            X_history = X_history.to(device)\n",
    "            X_target = X_target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(X_history)\n",
    "            loss = criterion(pred, X_target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_history, X_target in val_loader:\n",
    "                X_history = X_history.to(device)\n",
    "                X_target = X_target.to(device)\n",
    "                \n",
    "                pred = model(X_history)\n",
    "                loss = criterion(pred, X_target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.6f}, Val: {val_loss:.6f} - {epoch_time:.1f}s\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Setup and Train\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NORMALIZING DATA\n",
      "======================================================================\n",
      "Original data shape: (4320, 5, 6)\n",
      "Original stats - Mean: 176.09, Std: 373.40\n",
      "Original range: [0.00, 1041.02]\n",
      "WindSpeed    - Original: [0.00, 48.95], Normalized: [-2.26, 4.80]\n",
      "AirTemp      - Original: [5.28, 22.96], Normalized: [-3.23, 3.67]\n",
      "Pressure     - Original: [900.00, 1041.02], Normalized: [-7.87, 2.14]\n",
      "WaveHeight   - Original: [0.12, 12.54], Normalized: [-1.44, 5.41]\n",
      "Hmax         - Original: [0.00, 23.12], Normalized: [-1.46, 6.25]\n",
      "Tp           - Original: [2.34, 22.27], Normalized: [-2.10, 3.87]\n",
      "\n",
      "Normalized stats - Mean: 0.000000, Std: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX: NORMALIZE DATA BEFORE TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NORMALIZING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Your current data\n",
    "recent_size = 30 * 24 * 6  # 6 months\n",
    "X_tensor_recent = X_tensor[-recent_size:]\n",
    "\n",
    "print(f\"Original data shape: {X_tensor_recent.shape}\")\n",
    "print(f\"Original stats - Mean: {X_tensor_recent.mean():.2f}, Std: {X_tensor_recent.std():.2f}\")\n",
    "print(f\"Original range: [{X_tensor_recent.min():.2f}, {X_tensor_recent.max():.2f}]\")\n",
    "\n",
    "# NORMALIZE PROPERLY\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_timesteps, n_buoys, n_features = X_tensor_recent.shape\n",
    "\n",
    "# Store scalers for each feature (to denormalize later)\n",
    "scalers = []\n",
    "X_normalized = np.zeros_like(X_tensor_recent)\n",
    "\n",
    "feature_names = ['WindSpeed', 'AirTemp', 'Pressure', 'WaveHeight', 'Hmax', 'Tp']\n",
    "\n",
    "for f_idx in range(n_features):\n",
    "    # Extract all values for this feature\n",
    "    feature_values = X_tensor_recent[:, :, f_idx].reshape(-1, 1)\n",
    "    \n",
    "    # Fit scaler\n",
    "    scaler = StandardScaler()\n",
    "    normalized_values = scaler.fit_transform(feature_values)\n",
    "    \n",
    "    # Reshape back\n",
    "    X_normalized[:, :, f_idx] = normalized_values.reshape(n_timesteps, n_buoys)\n",
    "    \n",
    "    scalers.append(scaler)\n",
    "    \n",
    "    print(f\"{feature_names[f_idx]:12s} - Original: [{feature_values.min():.2f}, {feature_values.max():.2f}], \"\n",
    "          f\"Normalized: [{normalized_values.min():.2f}, {normalized_values.max():.2f}]\")\n",
    "\n",
    "print(f\"\\nNormalized stats - Mean: {X_normalized.mean():.6f}, Std: {X_normalized.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SimpleBuoyDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Run this first\n",
    "print(SimpleBuoyDataset)  # Should show: <class '__main__.SimpleBuoyDataset'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_normalized type: <class 'numpy.ndarray'>\n",
      "X_normalized shape: (4320, 5, 6)\n",
      "X_normalized dtype: float64\n",
      "X_normalized size (MB): 1.04\n",
      "\n",
      "Mean: 0.000000\n",
      "Std: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Check what X_normalized is\n",
    "print(f\"X_normalized type: {type(X_normalized)}\")\n",
    "print(f\"X_normalized shape: {X_normalized.shape}\")\n",
    "print(f\"X_normalized dtype: {X_normalized.dtype}\")\n",
    "print(f\"X_normalized size (MB): {X_normalized.nbytes / 1e6:.2f}\")\n",
    "\n",
    "# Check if it's actually normalized\n",
    "print(f\"\\nMean: {X_normalized.mean():.6f}\")\n",
    "print(f\"Std: {X_normalized.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOW CREATE DATASET WITH NORMALIZED DATA\n",
    "# ============================================================================\n",
    "\n",
    "sequence_length = 4\n",
    "dataset = SimpleBuoyDataset(X_normalized, sequence_length=sequence_length)\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "\n",
    "# Split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_indices = list(range(train_size))\n",
    "val_indices = list(range(train_size, len(dataset)))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RETRAIN WITH NORMALIZED DATA\n",
    "# ============================================================================\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "model = SimpleLSTM(n_buoys=5, n_features=6, hidden_dim=128, num_layers=2)\n",
    "\n",
    "print(f\"\\nRetraining with normalized data...\")\n",
    "\n",
    "train_losses, val_losses = train_lstm(\n",
    "    model, train_loader, val_loader, \n",
    "    num_epochs=20, \n",
    "    lr=0.001,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DENORMALIZE PREDICTIONS FOR EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_history, X_target in val_loader:\n",
    "        X_history = X_history.to(device)\n",
    "        pred = model(X_history)\n",
    "        all_preds.append(pred.cpu().numpy())\n",
    "        all_targets.append(X_target.numpy())\n",
    "\n",
    "predictions_normalized = np.vstack(all_preds)\n",
    "actuals_normalized = np.vstack(all_targets)\n",
    "\n",
    "# Denormalize\n",
    "predictions = np.zeros_like(predictions_normalized)\n",
    "actuals = np.zeros_like(actuals_normalized)\n",
    "\n",
    "for f_idx in range(n_features):\n",
    "    # Predictions\n",
    "    pred_flat = predictions_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    pred_denorm = scalers[f_idx].inverse_transform(pred_flat)\n",
    "    predictions[:, :, f_idx] = pred_denorm.reshape(-1, n_buoys)\n",
    "    \n",
    "    # Actuals\n",
    "    actual_flat = actuals_normalized[:, :, f_idx].reshape(-1, 1)\n",
    "    actual_denorm = scalers[f_idx].inverse_transform(actual_flat)\n",
    "    actuals[:, :, f_idx] = actual_denorm.reshape(-1, n_buoys)\n",
    "\n",
    "print(f\"\\n✓ Predictions denormalized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RECALCULATE METRICS WITH DENORMALIZED DATA\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS (DENORMALIZED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "buoy_names = ['M2', 'M3', 'M4', 'M5', 'M6']\n",
    "\n",
    "for b_idx, buoy in enumerate(buoy_names):\n",
    "    print(f\"\\n{buoy}:\")\n",
    "    for f_idx, feature in enumerate(feature_names):\n",
    "        actual = actuals[:, b_idx, f_idx]\n",
    "        pred = predictions[:, b_idx, f_idx]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        r2 = r2_score(actual, pred)\n",
    "        \n",
    "        print(f\"  {feature:<15} RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLOT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot M5 Wind Speed\n",
    "b_idx = 3  # M5\n",
    "f_idx = 1  # WindSpeed\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_samples = min(168, len(actuals))  # Last week or all data\n",
    "\n",
    "plt.plot(actuals[-plot_samples:, b_idx, f_idx], 'o-', label='Actual', markersize=4, linewidth=2)\n",
    "plt.plot(predictions[-plot_samples:, b_idx, f_idx], 's-', label='Predicted', markersize=3, alpha=0.7, linewidth=2)\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Wind Speed (knots)')\n",
    "plt.title('M5 Wind Speed - Predictions vs Actual')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check variance\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PREDICTION QUALITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prediction variance: {predictions[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Actual variance: {actuals[:, b_idx, f_idx].std():.4f}\")\n",
    "print(f\"Ratio: {predictions[:, b_idx, f_idx].std() / actuals[:, b_idx, f_idx].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
